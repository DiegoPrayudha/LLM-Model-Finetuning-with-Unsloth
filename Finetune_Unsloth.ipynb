{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install the Unsloth Python package."
      ],
      "metadata": {
        "id": "HaqETGrJX_Gq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25lHhZaIEawc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model and tokenizer using the FastLanguageModel function."
      ],
      "metadata": {
        "id": "MvHpt9QlYCbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Yellow-AI-NLP/komodo-7b-base\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = \"hf_**********\", # Get a token at https://huggingface.co/settings/tokens\n",
        ")"
      ],
      "metadata": {
        "id": "F4CYnj1JEkqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We add the LoRA (Low-Rank Adapter) to the model using the linear modules from the base model."
      ],
      "metadata": {
        "id": "DtCoyd6IYHUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "9_AMXJj1E8Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a prompt style to help achieve the desired results. A prompt consists of instructions, input, and output."
      ],
      "metadata": {
        "id": "WgO2NvxaYR_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_style = \"\"\"Di bawah ini terdapat sebuah instruksi yang menggambarkan sebuah tugas, disertai dengan sebuah masukan yang memberikan konteks lebih lanjut. Tulislah respons yang sesuai untuk menyelesaikan permintaan tersebut.\n",
        "\n",
        "### Instruksi:\n",
        "Anda adalah seorang asisten virtual Indonesia. Tugas Anda adalah memberikan penjelasan yang jelas dan ringkas berdasarkan pengetahuan anda. Pastikan respons Anda akurat, objektif, mudah dipahami, dan menggunakan bahasa Indonesia.\n",
        "\n",
        "### Masukan:\n",
        "{}\n",
        "\n",
        "### Respons:\n",
        "{}\"\"\""
      ],
      "metadata": {
        "id": "jjSjivWaE_Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for input, output in zip(inputs, outputs):\n",
        "        text = prompt_style.format(input, output)\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }"
      ],
      "metadata": {
        "id": "l7hfq1sFFQli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset and apply the predefined prompt format to it.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "07U7vupLYySs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# dataset = load_dataset(\"csv\", data_files=\"dataset-finetuning/merged_dataset.csv\", split=\"train[0:50000]\") # If your data from csv files\n",
        "\n",
        "dataset = load_dataset(\"Ichsan2895/alpaca-gpt4-indonesian\", split=\"train\")\n",
        "dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        ")\n",
        "dataset[\"text\"][0]"
      ],
      "metadata": {
        "id": "Gb4ZnAoWFdCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the trainer with the model, tokenizer, max sequence length, and training arguments."
      ],
      "metadata": {
        "id": "CRot7ZX8ZBU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 8,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 3, # Set this for 1 full training run.\n",
        "        max_steps = 100,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 42,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "NtK-IJufGYkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "lVRV30UhZD45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "Gb-OHC8eGbJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "NryANvmMZNzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the training is a LoRA adapter that can be saved locally."
      ],
      "metadata": {
        "id": "f-krqbcBGg5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Local Saving\n",
        "model.save_pretrained(\"/unsloth-komodo-7B-v2\")\n",
        "tokenizer.save_pretrained(\"/unsloth-komodo-7B-v2\")"
      ],
      "metadata": {
        "id": "d5QaIyxWGebu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also merge the adapter with the base model and save it locally."
      ],
      "metadata": {
        "id": "KfvqCpN3G4A_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained_merged(\"/unsloth-komodo-7B-v2\", tokenizer, save_method = \"merged_16bit\",)"
      ],
      "metadata": {
        "id": "A920O8vRGpXm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also push the adapter model or the merged model to Hugging Face."
      ],
      "metadata": {
        "id": "F5SemQRrHE2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub_merged(\"digo-prayudha/unsloth-komodo-7B-v2\", tokenizer, save_method = \"merged_16bit\", token = \"hf_*******\") # Merged Model\n",
        "model.push_to_hub_merged(\"digo-prayudha/unsloth-komodo-7B-v2\", tokenizer, save_method = \"lora\", token = \"hf_*******\") # Adapter Model"
      ],
      "metadata": {
        "id": "Scg6zEaeHCOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also save it in the GGUF format."
      ],
      "metadata": {
        "id": "4MFaEZIDHnK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub_gguf(\n",
        "    \"unsloth-llama-3.2-1b-gguf\", # Change hf to your username!\n",
        "    tokenizer,\n",
        "    quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "    token = \"hf_*********\",\n",
        ")"
      ],
      "metadata": {
        "id": "9DvBttphHprM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}